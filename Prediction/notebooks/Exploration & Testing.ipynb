{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.anomaly import prepare_data, fetch_anomalies_and_preceding_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from json import load, dumps\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Anomaly Detection\n",
    "#\n",
    "# - Create a 'vitality' feature, which is an average of all the vitality metrics.\n",
    "# - Plot a graph of this over time\n",
    "# - Determine the standard deviation & mean\n",
    "# - Identify specific points which fall out of (mean +/- sigma), and return the 7 day period immediately preceeding this (separate function).\n",
    "#     - One function which detects anomalies, and another function which takes the anomalies, and uses the date feature to extract 7 (or otherwise specified) periods of measured metric data immediately preceding the anomaly's date.\n",
    "#     - Quantify changes in correlated metrics (% change in the last few days compared to a global average or baseline)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Data preparation\n",
    "# data = pd.read_csv('sample_agg_health_and_checkin_data.csv')\n",
    "#\n",
    "# pd.read_json('sampleAggHealthAndCheckinData.json')\n",
    "\n",
    "vitality_metrics = [\"generalFeeling\", \"mood\", \"focus\", \"energy\"]\n",
    "measured_metrics = []\n",
    "\n",
    "# Given a list of health data, engineers the 'vitality' feature, which is the average of all vitality metrics recorded\n",
    "# for a given day, excluding '0' or NaN values for each row considered when applying a column-wise mean.\n",
    "# The result is an array of shape (-1, 2), where each row contains the original data frame index, so that we can\n",
    "# re-merge at a later stage and extract the measured_metrics, as well as the actual vitality for the day.\n",
    "def summarise_vitality(data, vitality_metrics):\n",
    "    vitality = np.array([ np.mean(x[np.logical_not(np.isnan(x))]) for x in data[vitality_metrics].values ]).reshape(-1, 1)\n",
    "    indeces = np.array(data.index.to_series().values).reshape(-1, 1)\n",
    "    return np.append(indeces, vitality, axis=1)\n",
    "\n",
    "\n",
    "# Given a pd data frame containing aggregated health data, creates the 'vitality'\n",
    "# feature, and interpolates weight to remove NaN values.\n",
    "def prepare_data(data):\n",
    "    # print(type(data))\n",
    "    if type(data) != pd.core.frame.DataFrame:\n",
    "        try:\n",
    "            # print(\"Attempting to read from json.\")\n",
    "            data = pd.read_json(data)\n",
    "        except:\n",
    "            log(\"Failed to parse data as JSON.\")\n",
    "            try:\n",
    "                # print(\"Attempting to read from csv.\")\n",
    "                data = pd.read_csv(data)\n",
    "            except:\n",
    "                log(\"Failed to parse data as CSV.\")\n",
    "\n",
    "                try:\n",
    "                    data = pd.DataFrame.from_dict(data)\n",
    "                except:\n",
    "                    log(\"Failed to parse data as dict.\")\n",
    "                    pass\n",
    "\n",
    "    # print(data)\n",
    "    assert type(data) is pd.core.frame.DataFrame\n",
    "\n",
    "    global measured_metrics\n",
    "\n",
    "    # Remove 'attributesAndCounts' if present.\n",
    "    if \"attributesAndCounts\" in data.columns:\n",
    "        data = data.drop(\"attributesAndCounts\", axis=1)\n",
    "\n",
    "    # Reassign measured_metrics to be used across the package.\n",
    "    measured_metrics = [col for col in data.columns.drop('startOfDate') if col not in vitality_metrics]\n",
    "\n",
    "    data_df = data.copy()\n",
    "\n",
    "    vitality = summarise_vitality(data, vitality_metrics)\n",
    "\n",
    "    # Add vitality to the data.\n",
    "    data_df[\"vitality\"] = vitality[:, 1]\n",
    "\n",
    "    # Replace all '0' values in weight by NaN values so that we can interpolate.\n",
    "    # Fill in all missing NaN weight values by linearly interpolating.\n",
    "    # data_df.loc[data_df[\"weight\"] == 0, \"weight\"] = None\n",
    "    data_df[\"weight\"] = data_df[\"weight\"].interpolate(limit_direction='both')\n",
    "    data_df.loc[data_df[\"weight\"] == 0, \"weight\"] = None\n",
    "    data_df[\"weight\"] = data_df[\"weight\"].interpolate(limit_direction='both')\n",
    "    \n",
    "    # Drop all NaN rows.\n",
    "    data_df = data_df.dropna()\n",
    "    \n",
    "    # Remove rows which have not been enriched with added health data (e.g. Apple Watch was not worn that day).\n",
    "    data_df = data_df.loc[data_df[\"basalEnergyBurned\"] != 0]\n",
    "\n",
    "    return data_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# # Detecting anomalies.\n",
    "# For each anomaly detected, return the immediate 'X' day period preceding the occurrance of the anomaly.\n",
    "# We merge the vitality for each day with all the other metrics observed or measured on a particular day / row.\n",
    "def get_data_preceding_anomaly(data, anomaly_idx, preceding_num_days=7):\n",
    "    return data.iloc[(anomaly_idx-preceding_num_days if anomaly_idx > preceding_num_days else 0):anomaly_idx]\n",
    "\n",
    "# For each measured metric preceding an anomaly, find the most correlated features which contribute to the anomaly itself.\n",
    "# For this, we will need to append 'vitality' as an extra column on the pandas dataframe.\n",
    "def get_most_correlated_complement(corr_matrix, metric):\n",
    "    most_corr_metric, most_corr_val = (\"\", 0)\n",
    "    for colName, value in corr_matrix[metric].iteritems():\n",
    "        if (colName == metric or colName in vitality_metrics):\n",
    "            continue\n",
    "        if (abs(value) > abs(most_corr_val)):\n",
    "            most_corr_metric, most_corr_val = (colName, value)\n",
    "    return (most_corr_metric, most_corr_val)\n",
    "\n",
    "# Given a correlation matrix and a desired metric, returns a list of metrics and their correlation\n",
    "# scores, sorted in terms of the highest correlation.\n",
    "def fetch_most_correlated_to(data_df, desired_metric):\n",
    "#     print(data_df, desired_metric)\n",
    "#     corr_matrix = data.corr()\n",
    "    metrics_to_select = measured_metrics + [desired_metric]\n",
    "#     print(f'Metrics to select: {metrics_to_select}')\n",
    "#     print(f'Subset of measured {data_df[measured_metrics].columns}')\n",
    "#     print(f'Subset of selected {data_df[metrics_to_select].columns}')\n",
    "#     print(f'Correlation matrix for metrics_to_select {data_df[metrics_to_select].corr()}')\n",
    "    correlation_matrix = data_df[metrics_to_select].corr()[desired_metric]\n",
    "    correlation_matrix_df = pd.DataFrame(correlation_matrix)\n",
    "\n",
    "#     print('Correlation Matrix', correlation_matrix_df)\n",
    "\n",
    "    sorted_abs_corr_matrix_df = pd.DataFrame(correlation_matrix.abs()).sort_values(desired_metric, ascending=False)\n",
    "\n",
    "#     print('Sorted (abs) corr matrix', sorted_abs_corr_matrix_df)\n",
    "\n",
    "    # After sorting the correlation matrix using absolute values, we need to 'merge' back in the negative\n",
    "    # values wherever they also apply. We keep all key in the sorted_abs_corr_matrix so that we can\n",
    "    # preserve the index ordering which we computed during the sort method applied above.\n",
    "    sorted_corr_matrix_df = sorted_abs_corr_matrix_df.join(other=correlation_matrix_df, on=sorted_abs_corr_matrix_df.index, how='left', lsuffix=\"_abs\").drop(f'{desired_metric}_abs', axis=1)\n",
    "\n",
    "#     print(\"Sorted corr matrix\", sorted_corr_matrix_df)\n",
    "\n",
    "    # Return all corrleated metrics, excluding 'vitality' which is trivially '1' by default.\n",
    "    return sorted_corr_matrix_df\n",
    "\n",
    "# Given a data frame, determines the % change in each column when compared to the global mean, as well as the\n",
    "# start of the subset period, by fitting a line and determining the % change between the first and last point\n",
    "# of the fitted line.\n",
    "def determine_global_percentage_change_for_subset(all_data, subset_data):\n",
    "    return pd.DataFrame((subset_data.mean() / all_data.mean()) - 1)\n",
    "\n",
    "# Determines the local percentage change for a subset of the data, by fitting a line of best fit and determining\n",
    "# the relative change between the first and the last value.\n",
    "def determine_local_percentage_change_for_subset(subset_data):\n",
    "\n",
    "    # if subset_data.empty:\n",
    "    #     return subset_data\n",
    "\n",
    "    x_mean = subset_data.index.values.mean()\n",
    "    x_centered = (subset_data.index.values - x_mean)\n",
    "\n",
    "    y_mean = subset_data.mean()\n",
    "    y_centered = subset_data.drop(\"startOfDate\", axis=1) - y_mean\n",
    "\n",
    "    m = (x_centered.reshape(-1, 1) * y_centered).sum() / (x_centered ** 2).sum()\n",
    "\n",
    "    b = y_mean - m * x_mean\n",
    "\n",
    "    lobf_first_point = m * subset_data.index.values[0] + b\n",
    "    lobf_last_point = m * subset_data.index.values[-1] + b\n",
    "\n",
    "    local_percentage_change = lobf_last_point / lobf_first_point\n",
    "\n",
    "    return pd.DataFrame(local_percentage_change - 1)\n",
    "\n",
    "# Obtains correlated metrics along with the % change in each correlated metric from the global average.\n",
    "# These are sorted by an 'importance' ranking which weights the correlation by the percentage change.\n",
    "# Returns all metrics with an importance greater than or equal to 0.4.\n",
    "def fetch_most_important_metrics_related_to(all_data, subset_data, desired_metric, importance_threshold=0.4):\n",
    "\n",
    "    most_corr_metrics = fetch_most_correlated_to(subset_data, desired_metric)\n",
    "    global_percentage_change_for_subset = determine_global_percentage_change_for_subset(all_data, subset_data)\n",
    "    local_percentage_change_for_subset = determine_local_percentage_change_for_subset(subset_data)\n",
    "    joined_df = most_corr_metrics\n",
    "    joined_df[\"local_percentage_change\"] = local_percentage_change_for_subset\n",
    "    joined_df[\"global_percentage_change\"] = global_percentage_change_for_subset\n",
    "    joined_df.columns = [\"correlation\", \"local_percentage_change\", \"global_percentage_change\"]\n",
    "\n",
    "    # Drop the desired_metric column.\n",
    "    joined_df = joined_df.drop(desired_metric, axis=0)\n",
    "\n",
    "    # Add an 'importance' column which weights each correlated metric by the percentage change which occured\n",
    "    # (both globally, and locally).\n",
    "    #\n",
    "    # We take into account the percentage change, as a correlation in and of itself isn't as useful out side\n",
    "    # of the context of a recent change. Indication of recent change would enable awareness and allow us to take action.\n",
    "    joined_df['importance'] = joined_df[\"correlation\"].abs() * joined_df[\"global_percentage_change\"].abs() * joined_df[\"local_percentage_change\"].abs()\n",
    "    joined_df['importance'] = joined_df['importance'] / joined_df['importance'].max()\n",
    "\n",
    "    return joined_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Fetch the most correlated metrics, and display a comparison for each.\n",
    "def fetch_most_correlated_pairs(data):\n",
    "    corr = data.corr()\n",
    "    correlated_metrics = []\n",
    "    for col in corr.columns:\n",
    "        pair, corrVal = get_most_correlated_complement(corr, col)\n",
    "        if ((col, pair, corrVal) not in correlated_metrics or (pair, col, corrVal) not in correlated_metrics):\n",
    "            correlated_metrics.append((col, pair, corrVal))\n",
    "    return correlated_metrics\n",
    "\n",
    "# Compare metrics which are most correlated to the desired metric visually, displaying the values for both\n",
    "# metrics across time, as well as labelling the correlation between the two metrics.\n",
    "# def compare_most_correlated_to_visually(data, desired_metric, max_num_correlated_metrics=3):\n",
    "\n",
    "#     most_correlated_metrics = fetch_most_correlated_to(data, desired_metric)\n",
    "#     most_correlated_metrics = list(zip(most_correlated_metrics.index, most_correlated_metrics.values.reshape(-1)))[1:]\n",
    "#     if max_num_correlated_metrics > len(most_correlated_metrics):\n",
    "#         max_num_correlated_metrics = len(most_correlated_metrics)\n",
    "\n",
    "#     # Fetch the `max_num_correlated_metrics` top most correlated metrics, skipping the first which is\n",
    "#     # trivially the desired_metric itself.\n",
    "#     most_correlated_metrics = most_correlated_metrics[0:max_num_correlated_metrics]\n",
    "\n",
    "#     plot_idx = 0\n",
    "#     for correlated_metric, corr_score in most_correlated_metrics:\n",
    "#         plt.figure(plot_idx)\n",
    "#         compare_metrics(data, desired_metric, correlated_metric, corr_score=corr_score)\n",
    "#         plot_idx += 1\n",
    "\n",
    "# def compare_most_correlated_visually(data):\n",
    "#     correlated_metrics = fetch_most_correlated_pairs(data)\n",
    "#     plotIndex = 0\n",
    "#     for (metricOne, metricTwo, _) in correlated_metrics:\n",
    "#         plt.figure(plotIndex)\n",
    "#         plotIndex += 1\n",
    "#         compare_metrics(data, metricOne, metricTwo)\n",
    "\n",
    "# Plotting restingHeartRate against the 4 vitality indicators.\n",
    "# def compare_metrics(df, metricOne, metricTwo, corr_score=None):\n",
    "#     feelingVHR = df[[metricOne, metricTwo]].dropna()\n",
    "#     metricOneMax = feelingVHR[metricOne].max()\n",
    "#     metricTwoMax = feelingVHR[metricTwo].max()\n",
    "#     plt.plot(feelingVHR[metricOne] / metricOneMax)\n",
    "#     plt.plot(feelingVHR[metricTwo] / metricTwoMax)\n",
    "#     plt.xlabel('Day')\n",
    "#     plt.ylabel('Normalised value')\n",
    "#     if corr_score is not None:\n",
    "#         plt.text((feelingVHR.index.min() + feelingVHR.index.max()) / 2, 1, f'Correlation: {corr_score}', horizontalalignment='center', verticalalignment='center')\n",
    "#     plt.legend()\n",
    "\n",
    "# Given the data, fetch the anomalies and the preceding data as an array of dictionaries\n",
    "# {\n",
    "#     anomaly_idx,\n",
    "#     anomaly_value,\n",
    "#     preceding_data,\n",
    "#     most_important_metrics,\n",
    "#     most_important_preceding_data\n",
    "# }\n",
    "def fetch_anomalies_and_preceding_data(data, desired_metric=\"vitality\", preceding_num_days=7, std_deviations=1, importance_threshold=0.4):\n",
    "\n",
    "    if desired_metric is None:\n",
    "        desired_metric = \"vitality\"\n",
    "\n",
    "    if preceding_num_days is None:\n",
    "        preceding_num_days = 7\n",
    "\n",
    "    if std_deviations is None:\n",
    "        std_deviations = 1\n",
    "\n",
    "    if importance_threshold is None:\n",
    "        importance_threshold = 0.4\n",
    "\n",
    "    data = prepare_data(data)\n",
    "\n",
    "    anomalies = []\n",
    "\n",
    "    anomaly_idxs = detect_anomaly_indeces(data=data, desired_metric=desired_metric, std_deviations=std_deviations)\n",
    "\n",
    "    anomaly_values = [data[desired_metric].iloc[idx] for idx in anomaly_idxs]\n",
    "\n",
    "    for anomaly_index, anomaly_value in zip(anomaly_idxs, anomaly_values):\n",
    "        preceding_data = get_data_preceding_anomaly(data, anomaly_index, preceding_num_days=preceding_num_days)\n",
    "\n",
    "        # If the preceding data is empty for whatever reason, skip the current anomaly.\n",
    "        if preceding_data.empty:\n",
    "            log(f'Preceding data empty for anomaly index {anomaly_index} and value {anomaly_value}.')\n",
    "            continue\n",
    "\n",
    "        important_metrics = fetch_most_important_metrics_related_to(data, preceding_data, desired_metric)\n",
    "        most_important_metrics = important_metrics.loc[important_metrics[\"importance\"] >= 0.4]\n",
    "        most_important_preceding_data = preceding_data[most_important_metrics.index.values]\n",
    "\n",
    "        anomalies.append({\n",
    "            \"desired_metric\": desired_metric,\n",
    "            \"anomaly_index\": int(anomaly_index),\n",
    "            \"anomaly_value\": float(anomaly_value),\n",
    "            \"preceding_data\": preceding_data.to_dict(),\n",
    "            \"most_important_metrics\": most_important_metrics.to_dict(),\n",
    "            \"most_important_preceding_data\": most_important_preceding_data.to_dict()\n",
    "        })\n",
    "\n",
    "    return anomalies\n",
    "\n",
    "def log(message):\n",
    "    print(f'ANOMLAY | {message}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"../data/sample_data.json\")\n",
    "# with open('./sample_data.json', 'r+') as json_file:\n",
    "#     json_dict = load(json_file)\n",
    "# #     print(json_dict[\"result\"])\n",
    "#     json_file.seek(0)\n",
    "#     json_file.truncate(0)\n",
    "#     json_file.write(dumps(json_dict[\"result\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prepped = prepare_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                  startOfDate  generalFeeling      mood    energy     focus  \\\n26   2020-05-18T00:00:00.000Z        3.000000  4.000000  3.000000  3.000000   \n38   2020-04-09T00:00:00.000Z        2.000000  2.000000  2.000000  2.000000   \n39   2020-04-08T00:00:00.000Z        2.000000  2.000000  2.000000  2.000000   \n40   2020-04-07T00:00:00.000Z        2.500000  3.000000  2.500000  2.500000   \n42   2020-04-05T00:00:00.000Z        3.000000  3.000000  2.500000  3.000000   \n44   2020-03-24T00:00:00.000Z        3.000000  3.000000  3.000000  2.000000   \n51   2020-03-12T00:00:00.000Z        1.000000  2.000000  1.000000  2.000000   \n54   2020-03-05T00:00:00.000Z        4.000000  4.000000  3.666667  3.500000   \n56   2020-03-03T00:00:00.000Z        5.000000  5.000000  4.500000  4.000000   \n57   2020-03-01T00:00:00.000Z        2.500000  3.000000  2.500000  3.000000   \n60   2020-02-26T00:00:00.000Z        1.500000  2.500000  1.500000  2.500000   \n61   2020-02-25T00:00:00.000Z        2.500000  3.000000  2.500000  2.500000   \n69   2020-02-13T00:00:00.000Z        3.000000  3.000000  3.000000  2.666667   \n78   2020-01-24T00:00:00.000Z        3.500000  3.500000  3.500000  3.500000   \n82   2020-01-17T00:00:00.000Z        3.250000  3.000000  2.666667  3.000000   \n86   2020-01-13T00:00:00.000Z        3.000000  3.000000  3.000000  2.000000   \n87   2020-01-10T00:00:00.000Z        1.000000  1.000000  1.000000  1.000000   \n88   2020-01-06T00:00:00.000Z        2.000000  1.333333  1.666667  1.500000   \n90   2020-01-04T00:00:00.000Z        2.500000  2.500000  2.500000  2.000000   \n92   2019-12-31T00:00:00.000Z        3.000000  3.500000  2.750000  2.500000   \n94   2019-12-28T00:00:00.000Z        3.750000  4.250000  3.500000  3.666667   \n95   2019-12-27T00:00:00.000Z        3.200000  3.000000  2.600000  2.000000   \n113  2019-12-08T00:00:00.000Z        2.750000  2.500000  2.500000  2.750000   \n114  2019-12-07T00:00:00.000Z        2.333333  3.000000  2.666667  3.000000   \n\n     activeEnergyBurned  basalEnergyBurned  caloricIntake  \\\n26              179.670           1303.657            0.0   \n38              246.898           1447.999            0.0   \n39              138.352           1610.592            0.0   \n40              203.119           1610.662            0.0   \n42              271.611           1491.387            0.0   \n44              221.724           1631.619            0.0   \n51              214.771           1645.249            0.0   \n54               15.157            442.473            0.0   \n56              213.533            586.183            0.0   \n57               27.204            643.883            0.0   \n60              612.294           1665.710            0.0   \n61              469.203           1226.774            0.0   \n69              251.757            828.328            0.0   \n78              287.717           1313.961            0.0   \n82              420.411            775.827            0.0   \n86              594.853           1204.580            0.0   \n87              478.770           1489.498            0.0   \n88              445.536            631.585            0.0   \n90              359.893           1490.037            0.0   \n92              103.321           1047.321            0.0   \n94              495.975           1301.727            0.0   \n95               88.409            946.513            0.0   \n113             633.587           1721.925            0.0   \n114             393.391           1631.346            0.0   \n\n     dietaryCarbohydrates  dietaryFats  dietaryProtein        hrv  \\\n26                    0.0          0.0             0.0  77.330400   \n38                    0.0          0.0             0.0  66.439715   \n39                    0.0          0.0             0.0  70.087394   \n40                    0.0          0.0             0.0  63.690593   \n42                    0.0          0.0             0.0  49.691969   \n44                    0.0          0.0             0.0  62.045726   \n51                    0.0          0.0             0.0  66.670102   \n54                    0.0          0.0             0.0  72.246145   \n56                    0.0          0.0             0.0  66.624398   \n57                    0.0          0.0             0.0  54.056139   \n60                    0.0          0.0             0.0  58.844023   \n61                    0.0          0.0             0.0  74.512284   \n69                    0.0          0.0             0.0  65.614102   \n78                    0.0          0.0             0.0   0.000000   \n82                    0.0          0.0             0.0  60.593315   \n86                    0.0          0.0             0.0  53.305466   \n87                    0.0          0.0             0.0  46.134897   \n88                    0.0          0.0             0.0  62.086506   \n90                    0.0          0.0             0.0  45.270607   \n92                    0.0          0.0             0.0  64.890650   \n94                    0.0          0.0             0.0  48.883789   \n95                    0.0          0.0             0.0  63.009947   \n113                   0.0          0.0             0.0  45.684179   \n114                   0.0          0.0             0.0  70.959694   \n\n     lowHeartRateEvents  restingHeartRate  sleepHours     weight  vitality  \n26                  1.0              40.0    8.183194  59.746013  3.250000  \n38                  1.0              40.0    7.252083  59.800052  2.000000  \n39                 11.0              36.0   10.357500  59.600121  2.000000  \n40                  5.0              35.0    8.752500  59.400190  2.625000  \n42                  3.0              35.0    7.500000  59.600221  2.875000  \n44                  3.0              40.0    6.158056  59.800105  2.750000  \n51                  2.0              40.0    8.316944  60.149902  1.500000  \n54                  3.0              39.0    0.000000  60.866659  3.791667  \n56                  0.0              41.0    7.478056  61.000016  4.625000  \n57                  0.0              41.0    0.000000  60.799980  2.750000  \n60                  3.0              40.0    6.847083  60.699959  2.000000  \n61                  1.0              40.0    7.379722  60.749967  2.625000  \n69                  5.0              39.0    9.127222  61.749912  2.916667  \n78                  0.0               0.0    0.000000  62.000047  3.500000  \n82                  1.0              40.0    7.519583  62.050150  2.979167  \n86                  4.0              39.0    7.726111  61.149921  2.750000  \n87                  2.0              41.0    9.378472  61.699899  1.000000  \n88                  0.0              41.0    7.438750  61.718667  1.625000  \n90                  0.0              47.0    8.873194  61.756202  2.375000  \n92                  1.0              41.0    9.514028  61.831272  2.937500  \n94                  0.0              52.0    0.000000  61.868806  3.791667  \n95                  6.0              37.0    9.448333  61.887574  2.700000  \n113                 1.0               0.0    0.000000  61.100254  2.625000  \n114                 7.0              37.0    9.800556  61.100254  2.750000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>startOfDate</th>\n      <th>generalFeeling</th>\n      <th>mood</th>\n      <th>energy</th>\n      <th>focus</th>\n      <th>activeEnergyBurned</th>\n      <th>basalEnergyBurned</th>\n      <th>caloricIntake</th>\n      <th>dietaryCarbohydrates</th>\n      <th>dietaryFats</th>\n      <th>dietaryProtein</th>\n      <th>hrv</th>\n      <th>lowHeartRateEvents</th>\n      <th>restingHeartRate</th>\n      <th>sleepHours</th>\n      <th>weight</th>\n      <th>vitality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>26</th>\n      <td>2020-05-18T00:00:00.000Z</td>\n      <td>3.000000</td>\n      <td>4.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>179.670</td>\n      <td>1303.657</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>77.330400</td>\n      <td>1.0</td>\n      <td>40.0</td>\n      <td>8.183194</td>\n      <td>59.746013</td>\n      <td>3.250000</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>2020-04-09T00:00:00.000Z</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>246.898</td>\n      <td>1447.999</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>66.439715</td>\n      <td>1.0</td>\n      <td>40.0</td>\n      <td>7.252083</td>\n      <td>59.800052</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>2020-04-08T00:00:00.000Z</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>138.352</td>\n      <td>1610.592</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>70.087394</td>\n      <td>11.0</td>\n      <td>36.0</td>\n      <td>10.357500</td>\n      <td>59.600121</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>2020-04-07T00:00:00.000Z</td>\n      <td>2.500000</td>\n      <td>3.000000</td>\n      <td>2.500000</td>\n      <td>2.500000</td>\n      <td>203.119</td>\n      <td>1610.662</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>63.690593</td>\n      <td>5.0</td>\n      <td>35.0</td>\n      <td>8.752500</td>\n      <td>59.400190</td>\n      <td>2.625000</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>2020-04-05T00:00:00.000Z</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>2.500000</td>\n      <td>3.000000</td>\n      <td>271.611</td>\n      <td>1491.387</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>49.691969</td>\n      <td>3.0</td>\n      <td>35.0</td>\n      <td>7.500000</td>\n      <td>59.600221</td>\n      <td>2.875000</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>2020-03-24T00:00:00.000Z</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>221.724</td>\n      <td>1631.619</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>62.045726</td>\n      <td>3.0</td>\n      <td>40.0</td>\n      <td>6.158056</td>\n      <td>59.800105</td>\n      <td>2.750000</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>2020-03-12T00:00:00.000Z</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>214.771</td>\n      <td>1645.249</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>66.670102</td>\n      <td>2.0</td>\n      <td>40.0</td>\n      <td>8.316944</td>\n      <td>60.149902</td>\n      <td>1.500000</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>2020-03-05T00:00:00.000Z</td>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>3.666667</td>\n      <td>3.500000</td>\n      <td>15.157</td>\n      <td>442.473</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>72.246145</td>\n      <td>3.0</td>\n      <td>39.0</td>\n      <td>0.000000</td>\n      <td>60.866659</td>\n      <td>3.791667</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>2020-03-03T00:00:00.000Z</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>4.500000</td>\n      <td>4.000000</td>\n      <td>213.533</td>\n      <td>586.183</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>66.624398</td>\n      <td>0.0</td>\n      <td>41.0</td>\n      <td>7.478056</td>\n      <td>61.000016</td>\n      <td>4.625000</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>2020-03-01T00:00:00.000Z</td>\n      <td>2.500000</td>\n      <td>3.000000</td>\n      <td>2.500000</td>\n      <td>3.000000</td>\n      <td>27.204</td>\n      <td>643.883</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>54.056139</td>\n      <td>0.0</td>\n      <td>41.0</td>\n      <td>0.000000</td>\n      <td>60.799980</td>\n      <td>2.750000</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>2020-02-26T00:00:00.000Z</td>\n      <td>1.500000</td>\n      <td>2.500000</td>\n      <td>1.500000</td>\n      <td>2.500000</td>\n      <td>612.294</td>\n      <td>1665.710</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>58.844023</td>\n      <td>3.0</td>\n      <td>40.0</td>\n      <td>6.847083</td>\n      <td>60.699959</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>2020-02-25T00:00:00.000Z</td>\n      <td>2.500000</td>\n      <td>3.000000</td>\n      <td>2.500000</td>\n      <td>2.500000</td>\n      <td>469.203</td>\n      <td>1226.774</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>74.512284</td>\n      <td>1.0</td>\n      <td>40.0</td>\n      <td>7.379722</td>\n      <td>60.749967</td>\n      <td>2.625000</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>2020-02-13T00:00:00.000Z</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>2.666667</td>\n      <td>251.757</td>\n      <td>828.328</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>65.614102</td>\n      <td>5.0</td>\n      <td>39.0</td>\n      <td>9.127222</td>\n      <td>61.749912</td>\n      <td>2.916667</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>2020-01-24T00:00:00.000Z</td>\n      <td>3.500000</td>\n      <td>3.500000</td>\n      <td>3.500000</td>\n      <td>3.500000</td>\n      <td>287.717</td>\n      <td>1313.961</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>62.000047</td>\n      <td>3.500000</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>2020-01-17T00:00:00.000Z</td>\n      <td>3.250000</td>\n      <td>3.000000</td>\n      <td>2.666667</td>\n      <td>3.000000</td>\n      <td>420.411</td>\n      <td>775.827</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>60.593315</td>\n      <td>1.0</td>\n      <td>40.0</td>\n      <td>7.519583</td>\n      <td>62.050150</td>\n      <td>2.979167</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>2020-01-13T00:00:00.000Z</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>594.853</td>\n      <td>1204.580</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>53.305466</td>\n      <td>4.0</td>\n      <td>39.0</td>\n      <td>7.726111</td>\n      <td>61.149921</td>\n      <td>2.750000</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>2020-01-10T00:00:00.000Z</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>478.770</td>\n      <td>1489.498</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>46.134897</td>\n      <td>2.0</td>\n      <td>41.0</td>\n      <td>9.378472</td>\n      <td>61.699899</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>2020-01-06T00:00:00.000Z</td>\n      <td>2.000000</td>\n      <td>1.333333</td>\n      <td>1.666667</td>\n      <td>1.500000</td>\n      <td>445.536</td>\n      <td>631.585</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>62.086506</td>\n      <td>0.0</td>\n      <td>41.0</td>\n      <td>7.438750</td>\n      <td>61.718667</td>\n      <td>1.625000</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>2020-01-04T00:00:00.000Z</td>\n      <td>2.500000</td>\n      <td>2.500000</td>\n      <td>2.500000</td>\n      <td>2.000000</td>\n      <td>359.893</td>\n      <td>1490.037</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>45.270607</td>\n      <td>0.0</td>\n      <td>47.0</td>\n      <td>8.873194</td>\n      <td>61.756202</td>\n      <td>2.375000</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>2019-12-31T00:00:00.000Z</td>\n      <td>3.000000</td>\n      <td>3.500000</td>\n      <td>2.750000</td>\n      <td>2.500000</td>\n      <td>103.321</td>\n      <td>1047.321</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>64.890650</td>\n      <td>1.0</td>\n      <td>41.0</td>\n      <td>9.514028</td>\n      <td>61.831272</td>\n      <td>2.937500</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>2019-12-28T00:00:00.000Z</td>\n      <td>3.750000</td>\n      <td>4.250000</td>\n      <td>3.500000</td>\n      <td>3.666667</td>\n      <td>495.975</td>\n      <td>1301.727</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>48.883789</td>\n      <td>0.0</td>\n      <td>52.0</td>\n      <td>0.000000</td>\n      <td>61.868806</td>\n      <td>3.791667</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>2019-12-27T00:00:00.000Z</td>\n      <td>3.200000</td>\n      <td>3.000000</td>\n      <td>2.600000</td>\n      <td>2.000000</td>\n      <td>88.409</td>\n      <td>946.513</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>63.009947</td>\n      <td>6.0</td>\n      <td>37.0</td>\n      <td>9.448333</td>\n      <td>61.887574</td>\n      <td>2.700000</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>2019-12-08T00:00:00.000Z</td>\n      <td>2.750000</td>\n      <td>2.500000</td>\n      <td>2.500000</td>\n      <td>2.750000</td>\n      <td>633.587</td>\n      <td>1721.925</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>45.684179</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>61.100254</td>\n      <td>2.625000</td>\n    </tr>\n    <tr>\n      <th>114</th>\n      <td>2019-12-07T00:00:00.000Z</td>\n      <td>2.333333</td>\n      <td>3.000000</td>\n      <td>2.666667</td>\n      <td>3.000000</td>\n      <td>393.391</td>\n      <td>1631.346</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>70.959694</td>\n      <td>7.0</td>\n      <td>37.0</td>\n      <td>9.800556</td>\n      <td>61.100254</td>\n      <td>2.750000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "data_prepped.loc[data_prepped[\"caloricIntake\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Anomaly Detection\n",
    "#\n",
    "# - Create a 'vitality' feature, which is an average of all the vitality metrics.\n",
    "# - Plot a graph of this over time\n",
    "# - Determine the standard deviation & mean\n",
    "# - Identify specific points which fall out of (mean +/- sigma), and return the 7 day period immediately preceeding this (separate function).\n",
    "#     - One function which detects anomalies, and another function which takes the anomalies, and uses the date feature to extract 7 (or otherwise specified) periods of measured metric data immediately preceding the anomaly's date.\n",
    "#     - Quantify changes in correlated metrics (% change in the last few days compared to a global average or baseline)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Data preparation\n",
    "# data = pd.read_csv('sample_agg_health_and_checkin_data.csv')\n",
    "#\n",
    "# pd.read_json('sampleAggHealthAndCheckinData.json')\n",
    "\n",
    "vitality_metrics = [\"generalFeeling\", \"mood\", \"focus\", \"energy\"]\n",
    "measured_metrics = []\n",
    "\n",
    "# Given a list of health data, engineers the 'vitality' feature, which is the average of all vitality metrics recorded\n",
    "# for a given day, excluding '0' or NaN values for each row considered when applying a column-wise mean.\n",
    "# The result is an array of shape (-1, 2), where each row contains the original data frame index, so that we can\n",
    "# re-merge at a later stage and extract the measured_metrics, as well as the actual vitality for the day.\n",
    "def summarise_vitality(data, vitality_metrics):\n",
    "    vitality = np.array([ np.mean(x[np.logical_not(np.isnan(x))]) for x in data[vitality_metrics].values ]).reshape(-1, 1)\n",
    "    indeces = np.array(data.index.to_series().values).reshape(-1, 1)\n",
    "    return np.append(indeces, vitality, axis=1)\n",
    "\n",
    "\n",
    "# Given a pd data frame containing aggregated health data, creates the 'vitality'\n",
    "# feature, and interpolates weight to remove NaN values.\n",
    "def prepare_data(data):\n",
    "    # print(type(data))\n",
    "    if type(data) != pd.core.frame.DataFrame:\n",
    "        try:\n",
    "            # print(\"Attempting to read from json.\")\n",
    "            data = pd.read_json(data)\n",
    "        except:\n",
    "            log(\"Failed to parse data as JSON.\")\n",
    "            try:\n",
    "                # print(\"Attempting to read from csv.\")\n",
    "                data = pd.read_csv(data)\n",
    "            except:\n",
    "                log(\"Failed to parse data as CSV.\")\n",
    "\n",
    "                try:\n",
    "                    data = pd.DataFrame.from_dict(data)\n",
    "                except:\n",
    "                    log(\"Failed to parse data as dict.\")\n",
    "                    pass\n",
    "\n",
    "    # print(data)\n",
    "    assert type(data) is pd.core.frame.DataFrame\n",
    "\n",
    "    global measured_metrics\n",
    "\n",
    "    # Remove 'attributesAndCounts' if present.\n",
    "    if \"attributesAndCounts\" in data.columns:\n",
    "        data = data.drop(\"attributesAndCounts\", axis=1)\n",
    "\n",
    "    # Reassign measured_metrics to be used across the package.\n",
    "    measured_metrics = [col for col in data.columns.drop('startOfDate') if col not in vitality_metrics]\n",
    "\n",
    "    data_df = data.copy()\n",
    "\n",
    "    vitality = summarise_vitality(data, vitality_metrics)\n",
    "\n",
    "    # Add vitality to the data.\n",
    "    data_df[\"vitality\"] = vitality[:, 1]\n",
    "\n",
    "    # Replace all '0' values in weight by NaN values so that we can interpolate.\n",
    "    # Fill in all missing NaN weight values by linearly interpolating.\n",
    "    # data_df.loc[data_df[\"weight\"] == 0, \"weight\"] = None\n",
    "    data_df[\"weight\"] = data_df[\"weight\"].interpolate(limit_direction='both')\n",
    "    data_df.loc[data_df[\"weight\"] == 0, \"weight\"] = None\n",
    "    data_df[\"weight\"] = data_df[\"weight\"].interpolate(limit_direction='both')\n",
    "    \n",
    "    # Drop all NaN rows.\n",
    "    data_df = data_df.dropna()\n",
    "    \n",
    "    # Remove rows which have not been enriched with added health data (e.g. Apple Watch was not worn that day).\n",
    "    data_df = data_df.loc[data_df[\"basalEnergyBurned\"] != 0]\n",
    "\n",
    "    return data_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# # Detecting anomalies.\n",
    "\n",
    "# Detects anomalies in the data by detecting values which fall outside of the\n",
    "# mu +/- sigma range, returning the indeces of the corresponding data frame\n",
    "# rows where the desired metric falls outside of this range.\n",
    "def detect_anomaly_indeces(data, desired_metric, std_deviations=1):\n",
    "\n",
    "    sigma = data[desired_metric].std()\n",
    "    mu = data[desired_metric].mean()\n",
    "\n",
    "    upper_bound = mu + sigma*std_deviations\n",
    "    lower_bound = mu - sigma*std_deviations\n",
    "\n",
    "    anomaly_indeces = np.where((data[desired_metric] != 0) & (data[desired_metric] < lower_bound) | (data[desired_metric] > upper_bound))\n",
    "    return anomaly_indeces[0]\n",
    "\n",
    "# For each measured metric preceding an anomaly, find the most correlated features which contribute to the anomaly itself.\n",
    "# For this, we will need to append 'vitality' as an extra column on the pandas dataframe.\n",
    "def get_most_correlated_complement(corr_matrix, metric):\n",
    "    most_corr_metric, most_corr_val = (\"\", 0)\n",
    "    for colName, value in corr_matrix[metric].iteritems():\n",
    "        if (colName == metric or colName in vitality_metrics):\n",
    "            continue\n",
    "        if (abs(value) > abs(most_corr_val)):\n",
    "            most_corr_metric, most_corr_val = (colName, value)\n",
    "    return (most_corr_metric, most_corr_val)\n",
    "\n",
    "# Given a correlation matrix and a desired metric, returns a list of metrics and their correlation\n",
    "# scores, sorted in terms of the highest correlation.\n",
    "def fetch_most_correlated_to(data_df, desired_metric):\n",
    "#     print(data_df, desired_metric)\n",
    "#     corr_matrix = data.corr()\n",
    "    metrics_to_select = measured_metrics + [desired_metric]\n",
    "#     print(f'Metrics to select: {metrics_to_select}')\n",
    "#     print(f'Subset of measured {data_df[measured_metrics].columns}')\n",
    "#     print(f'Subset of selected {data_df[metrics_to_select].columns}')\n",
    "#     print(f'Correlation matrix for metrics_to_select {data_df[metrics_to_select].corr()}')\n",
    "    correlation_matrix = data_df[metrics_to_select].corr()[desired_metric]\n",
    "    correlation_matrix_df = pd.DataFrame(correlation_matrix)\n",
    "\n",
    "#     print('Correlation Matrix', correlation_matrix_df)\n",
    "\n",
    "    sorted_abs_corr_matrix_df = pd.DataFrame(correlation_matrix.abs()).sort_values(desired_metric, ascending=False)\n",
    "\n",
    "#     print('Sorted (abs) corr matrix', sorted_abs_corr_matrix_df)\n",
    "\n",
    "    # After sorting the correlation matrix using absolute values, we need to 'merge' back in the negative\n",
    "    # values wherever they also apply. We keep all key in the sorted_abs_corr_matrix so that we can\n",
    "    # preserve the index ordering which we computed during the sort method applied above.\n",
    "    sorted_corr_matrix_df = sorted_abs_corr_matrix_df.join(other=correlation_matrix_df, on=sorted_abs_corr_matrix_df.index, how='left', lsuffix=\"_abs\").drop(f'{desired_metric}_abs', axis=1)\n",
    "\n",
    "#     print(\"Sorted corr matrix\", sorted_corr_matrix_df)\n",
    "\n",
    "    # Return all corrleated metrics, excluding 'vitality' which is trivially '1' by default.\n",
    "    return sorted_corr_matrix_df\n",
    "\n",
    "# Given a data frame, determines the % change in each column when compared to the global mean, as well as the\n",
    "# start of the subset period, by fitting a line and determining the % change between the first and last point\n",
    "# of the fitted line.\n",
    "def determine_global_percentage_change_for_subset(all_data, subset_data):\n",
    "    return pd.DataFrame((subset_data.mean() / all_data.mean()) - 1)\n",
    "\n",
    "# Determines the local percentage change for a subset of the data, by fitting a line of best fit and determining\n",
    "# the relative change between the first and the last value.\n",
    "def determine_local_percentage_change_for_subset(subset_data):\n",
    "\n",
    "    # if subset_data.empty:\n",
    "    #     return subset_data\n",
    "\n",
    "    x_mean = subset_data.index.values.mean()\n",
    "    x_centered = (subset_data.index.values - x_mean)\n",
    "\n",
    "    y_mean = subset_data.mean()\n",
    "    y_centered = subset_data.drop(\"startOfDate\", axis=1) - y_mean\n",
    "\n",
    "    m = (x_centered.reshape(-1, 1) * y_centered).sum() / (x_centered ** 2).sum()\n",
    "\n",
    "    b = y_mean - m * x_mean\n",
    "\n",
    "    lobf_first_point = m * subset_data.index.values[0] + b\n",
    "    lobf_last_point = m * subset_data.index.values[-1] + b\n",
    "\n",
    "    local_percentage_change = lobf_last_point / lobf_first_point\n",
    "\n",
    "    return pd.DataFrame(local_percentage_change - 1)\n",
    "\n",
    "# Obtains correlated metrics along with the % change in each correlated metric from the global average.\n",
    "# These are sorted by an 'importance' ranking which weights the correlation by the percentage change.\n",
    "# Returns all metrics with an importance greater than or equal to 0.4.\n",
    "def fetch_most_important_metrics_related_to(all_data, subset_data, desired_metric, importance_threshold=0.4):\n",
    "\n",
    "    most_corr_metrics = fetch_most_correlated_to(subset_data, desired_metric)\n",
    "    global_percentage_change_for_subset = determine_global_percentage_change_for_subset(all_data, subset_data)\n",
    "    local_percentage_change_for_subset = determine_local_percentage_change_for_subset(subset_data)\n",
    "    joined_df = most_corr_metrics\n",
    "    joined_df[\"local_percentage_change\"] = local_percentage_change_for_subset\n",
    "    joined_df[\"global_percentage_change\"] = global_percentage_change_for_subset\n",
    "    joined_df.columns = [\"correlation\", \"local_percentage_change\", \"global_percentage_change\"]\n",
    "\n",
    "    # Drop the desired_metric column.\n",
    "    joined_df = joined_df.drop(desired_metric, axis=0)\n",
    "\n",
    "    # Add an 'importance' column which weights each correlated metric by the percentage change which occured\n",
    "    # (both globally, and locally).\n",
    "    #\n",
    "    # We take into account the percentage change, as a correlation in and of itself isn't as useful out side\n",
    "    # of the context of a recent change. Indication of recent change would enable awareness and allow us to take action.\n",
    "    joined_df['importance'] = joined_df[\"correlation\"].abs() * joined_df[\"global_percentage_change\"].abs() * joined_df[\"local_percentage_change\"].abs()\n",
    "    joined_df['importance'] = joined_df['importance'] / joined_df['importance'].max()\n",
    "\n",
    "    return joined_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Fetch the most correlated metrics, and display a comparison for each.\n",
    "def fetch_most_correlated_pairs(data):\n",
    "    corr = data.corr()\n",
    "    correlated_metrics = []\n",
    "    for col in corr.columns:\n",
    "        pair, corrVal = get_most_correlated_complement(corr, col)\n",
    "        if ((col, pair, corrVal) not in correlated_metrics or (pair, col, corrVal) not in correlated_metrics):\n",
    "            correlated_metrics.append((col, pair, corrVal))\n",
    "    return correlated_metrics\n",
    "\n",
    "# Compare metrics which are most correlated to the desired metric visually, displaying the values for both\n",
    "# metrics across time, as well as labelling the correlation between the two metrics.\n",
    "# def compare_most_correlated_to_visually(data, desired_metric, max_num_correlated_metrics=3):\n",
    "\n",
    "#     most_correlated_metrics = fetch_most_correlated_to(data, desired_metric)\n",
    "#     most_correlated_metrics = list(zip(most_correlated_metrics.index, most_correlated_metrics.values.reshape(-1)))[1:]\n",
    "#     if max_num_correlated_metrics > len(most_correlated_metrics):\n",
    "#         max_num_correlated_metrics = len(most_correlated_metrics)\n",
    "\n",
    "#     # Fetch the `max_num_correlated_metrics` top most correlated metrics, skipping the first which is\n",
    "#     # trivially the desired_metric itself.\n",
    "#     most_correlated_metrics = most_correlated_metrics[0:max_num_correlated_metrics]\n",
    "\n",
    "#     plot_idx = 0\n",
    "#     for correlated_metric, corr_score in most_correlated_metrics:\n",
    "#         plt.figure(plot_idx)\n",
    "#         compare_metrics(data, desired_metric, correlated_metric, corr_score=corr_score)\n",
    "#         plot_idx += 1\n",
    "\n",
    "# def compare_most_correlated_visually(data):\n",
    "#     correlated_metrics = fetch_most_correlated_pairs(data)\n",
    "#     plotIndex = 0\n",
    "#     for (metricOne, metricTwo, _) in correlated_metrics:\n",
    "#         plt.figure(plotIndex)\n",
    "#         plotIndex += 1\n",
    "#         compare_metrics(data, metricOne, metricTwo)\n",
    "\n",
    "# Plotting restingHeartRate against the 4 vitality indicators.\n",
    "# def compare_metrics(df, metricOne, metricTwo, corr_score=None):\n",
    "#     feelingVHR = df[[metricOne, metricTwo]].dropna()\n",
    "#     metricOneMax = feelingVHR[metricOne].max()\n",
    "#     metricTwoMax = feelingVHR[metricTwo].max()\n",
    "#     plt.plot(feelingVHR[metricOne] / metricOneMax)\n",
    "#     plt.plot(feelingVHR[metricTwo] / metricTwoMax)\n",
    "#     plt.xlabel('Day')\n",
    "#     plt.ylabel('Normalised value')\n",
    "#     if corr_score is not None:\n",
    "#         plt.text((feelingVHR.index.min() + feelingVHR.index.max()) / 2, 1, f'Correlation: {corr_score}', horizontalalignment='center', verticalalignment='center')\n",
    "#     plt.legend()\n",
    "\n",
    "# Given the data, fetch the anomalies and the preceding data as an array of dictionaries\n",
    "# {\n",
    "#     anomaly_idx,\n",
    "#     anomaly_value,\n",
    "#     preceding_data,\n",
    "#     most_important_metrics,\n",
    "#     most_important_preceding_data\n",
    "# }\n",
    "def fetch_anomalies_and_preceding_data(data, desired_metric=\"vitality\", preceding_num_days=7, std_deviations=1, importance_threshold=0.4):\n",
    "\n",
    "    if desired_metric is None:\n",
    "        desired_metric = \"vitality\"\n",
    "\n",
    "    if preceding_num_days is None:\n",
    "        preceding_num_days = 7\n",
    "\n",
    "    if std_deviations is None:\n",
    "        std_deviations = 1\n",
    "\n",
    "    if importance_threshold is None:\n",
    "        importance_threshold = 0.4\n",
    "\n",
    "    data = prepare_data(data)\n",
    "\n",
    "    anomalies = []\n",
    "\n",
    "    anomaly_idxs = detect_anomaly_indeces(data=data, desired_metric=desired_metric, std_deviations=std_deviations)\n",
    "\n",
    "    anomaly_values = [data[desired_metric].iloc[idx] for idx in anomaly_idxs]\n",
    "\n",
    "    for anomaly_index, anomaly_value in zip(anomaly_idxs, anomaly_values):\n",
    "        preceding_data = get_data_preceding_anomaly(data, anomaly_index, preceding_num_days=preceding_num_days)\n",
    "\n",
    "        # If the preceding data is empty for whatever reason, skip the current anomaly.\n",
    "        if preceding_data.empty:\n",
    "            log(f'Preceding data empty for anomaly index {anomaly_index} and value {anomaly_value}.')\n",
    "            continue\n",
    "\n",
    "        important_metrics = fetch_most_important_metrics_related_to(data, preceding_data, desired_metric)\n",
    "        most_important_metrics = important_metrics.loc[important_metrics[\"importance\"] >= 0.4]\n",
    "        most_important_preceding_data = preceding_data[most_important_metrics.index.values]\n",
    "        \n",
    "        # If we do not have enough preceding data to cover the preceding number of days, then we should \n",
    "        # skip this particular result.\n",
    "#         if preceding_data\n",
    "\n",
    "        anomalies.append({\n",
    "            \"desired_metric\": desired_metric,\n",
    "            \"anomaly_index\": int(anomaly_index),\n",
    "            \"anomaly_value\": float(anomaly_value),\n",
    "            \"preceding_data\": preceding_data.to_dict(),\n",
    "            \"most_important_metrics\": most_important_metrics.to_dict(),\n",
    "            \"most_important_preceding_data\": most_important_preceding_data.to_dict()\n",
    "        })\n",
    "\n",
    "    return anomalies\n",
    "\n",
    "def log(message):\n",
    "    print(f'ANOMLAY | {message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([  8,  52,  65,  67,  68,  89,  90, 104, 112, 124, 126, 129, 140])"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "detect_anomaly_indeces(data, \"sleepHours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ANOMLAY | Preceding data empty for anomaly index 0 and value 1.75.\n"
    }
   ],
   "source": [
    "anomalies = fetch_anomalies_and_preceding_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([ 74, 108, 136]),)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "np.where(((data[\"caloricIntake\"] != 0) & (data[\"caloricIntake\"] > 3200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'2020-06-27T00:00:00.000Z'"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "data.iloc[9][\"startOfDate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each anomaly detected, return the immediate 'X' day period preceding the occurrance of the anomaly.\n",
    "# We merge the vitality for each day with all the other metrics observed or measured on a particular day / row.\n",
    "def get_data_preceding_anomaly(data, anomaly_idx, preceding_num_days=7):\n",
    "    return data.iloc[anomaly_idx: min(anomaly_idx+preceding_num_days, len(data))]\n",
    "#     return data.iloc[(anomaly_idx-preceding_num_days if anomaly_idx > preceding_num_days else 0):anomaly_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preceding_data = get_data_preceding_anomaly(data, 10, preceding_num_days=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                 startOfDate  generalFeeling  mood  energy  focus  \\\n10  2020-06-26T00:00:00.000Z             2.0   2.0     2.0    2.0   \n11  2020-06-25T00:00:00.000Z             1.0   1.0     1.0    1.0   \n12  2020-06-24T00:00:00.000Z             2.0   3.0     2.0    2.0   \n\n                                  attributesAndCounts  activeEnergyBurned  \\\n10  {'generalFeeling': 1, 'mood': 1, 'energy': 1, ...             749.137   \n11  {'generalFeeling': 1, 'mood': 1, 'energy': 1, ...             551.658   \n12  {'generalFeeling': 1, 'mood': 1, 'energy': 1, ...             265.060   \n\n    basalEnergyBurned  caloricIntake  dietaryCarbohydrates  dietaryFats  \\\n10           1598.732    1760.089325            112.762424    65.292268   \n11           1606.791    2174.395233            197.696106    70.343836   \n12           1582.153    1888.463425            143.513682    80.526786   \n\n    dietaryProtein        hrv  lowHeartRateEvents  restingHeartRate  \\\n10      157.223942  60.377090                 3.0              39.0   \n11      182.788260  69.829800                 1.0              40.0   \n12      143.098830  56.054286                18.0              37.0   \n\n    sleepHours     weight  \n10    9.622778   0.000000  \n11    8.825694   0.000000  \n12    9.568056  57.200267  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>startOfDate</th>\n      <th>generalFeeling</th>\n      <th>mood</th>\n      <th>energy</th>\n      <th>focus</th>\n      <th>attributesAndCounts</th>\n      <th>activeEnergyBurned</th>\n      <th>basalEnergyBurned</th>\n      <th>caloricIntake</th>\n      <th>dietaryCarbohydrates</th>\n      <th>dietaryFats</th>\n      <th>dietaryProtein</th>\n      <th>hrv</th>\n      <th>lowHeartRateEvents</th>\n      <th>restingHeartRate</th>\n      <th>sleepHours</th>\n      <th>weight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>2020-06-26T00:00:00.000Z</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>{'generalFeeling': 1, 'mood': 1, 'energy': 1, ...</td>\n      <td>749.137</td>\n      <td>1598.732</td>\n      <td>1760.089325</td>\n      <td>112.762424</td>\n      <td>65.292268</td>\n      <td>157.223942</td>\n      <td>60.377090</td>\n      <td>3.0</td>\n      <td>39.0</td>\n      <td>9.622778</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2020-06-25T00:00:00.000Z</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>{'generalFeeling': 1, 'mood': 1, 'energy': 1, ...</td>\n      <td>551.658</td>\n      <td>1606.791</td>\n      <td>2174.395233</td>\n      <td>197.696106</td>\n      <td>70.343836</td>\n      <td>182.788260</td>\n      <td>69.829800</td>\n      <td>1.0</td>\n      <td>40.0</td>\n      <td>8.825694</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2020-06-24T00:00:00.000Z</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>{'generalFeeling': 1, 'mood': 1, 'energy': 1, ...</td>\n      <td>265.060</td>\n      <td>1582.153</td>\n      <td>1888.463425</td>\n      <td>143.513682</td>\n      <td>80.526786</td>\n      <td>143.098830</td>\n      <td>56.054286</td>\n      <td>18.0</td>\n      <td>37.0</td>\n      <td>9.568056</td>\n      <td>57.200267</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "preceding_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>startOfDate</th>\n",
       "      <th>generalFeeling</th>\n",
       "      <th>mood</th>\n",
       "      <th>energy</th>\n",
       "      <th>focus</th>\n",
       "      <th>attributesAndCounts</th>\n",
       "      <th>activeEnergyBurned</th>\n",
       "      <th>basalEnergyBurned</th>\n",
       "      <th>caloricIntake</th>\n",
       "      <th>dietaryCarbohydrates</th>\n",
       "      <th>dietaryFats</th>\n",
       "      <th>dietaryProtein</th>\n",
       "      <th>hrv</th>\n",
       "      <th>lowHeartRateEvents</th>\n",
       "      <th>restingHeartRate</th>\n",
       "      <th>sleepHours</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020-06-24T00:00:00.000Z</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'generalFeeling': 1, 'mood': 1, 'energy': 1, ...</td>\n",
       "      <td>265.06</td>\n",
       "      <td>1582.15</td>\n",
       "      <td>1888.46</td>\n",
       "      <td>143.514</td>\n",
       "      <td>80.5268</td>\n",
       "      <td>143.099</td>\n",
       "      <td>56.0543</td>\n",
       "      <td>18</td>\n",
       "      <td>37</td>\n",
       "      <td>9.56806</td>\n",
       "      <td>57.2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020-06-23T00:00:00.000Z</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'generalFeeling': 1, 'mood': 1, 'energy': 1, ...</td>\n",
       "      <td>317.569</td>\n",
       "      <td>1582.59</td>\n",
       "      <td>1685.14</td>\n",
       "      <td>122.288</td>\n",
       "      <td>80.5868</td>\n",
       "      <td>108.147</td>\n",
       "      <td>54.1322</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>7.05458</td>\n",
       "      <td>57.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 startOfDate generalFeeling mood energy focus  \\\n",
       "12  2020-06-24T00:00:00.000Z              2    3      2     2   \n",
       "13  2020-06-23T00:00:00.000Z              1    1      1     1   \n",
       "\n",
       "                                  attributesAndCounts activeEnergyBurned  \\\n",
       "12  {'generalFeeling': 1, 'mood': 1, 'energy': 1, ...             265.06   \n",
       "13  {'generalFeeling': 1, 'mood': 1, 'energy': 1, ...            317.569   \n",
       "\n",
       "   basalEnergyBurned caloricIntake dietaryCarbohydrates dietaryFats  \\\n",
       "12           1582.15       1888.46              143.514     80.5268   \n",
       "13           1582.59       1685.14              122.288     80.5868   \n",
       "\n",
       "   dietaryProtein      hrv lowHeartRateEvents restingHeartRate sleepHours  \\\n",
       "12        143.099  56.0543                 18               37    9.56806   \n",
       "13        108.147  54.1322                  1               39    7.05458   \n",
       "\n",
       "     weight  \n",
       "12  57.2003  \n",
       "13     57.8  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preceding_data.where(preceding_data != 0, None).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a dataframe, creates a dict for each column as arrays rather than objects keyed by the index.\n",
    "def pandas_to_array_dict(df):\n",
    "    \n",
    "    output_dict = df.to_dict()\n",
    "    \n",
    "    for key in output_dict:\n",
    "        output_dict[key] = list(output_dict[key].values())\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverting the `most_imporant_metrics` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[{'correlation': 0.08448214307493142,\n  'local_percentage_change': 2.1000000000000005,\n  'global_percentage_change': 0.9467382328654004,\n  'importance': 1.0,\n  'metric': 'lowHeartRateEvents'},\n {'correlation': 0.3560415680299623,\n  'local_percentage_change': 0.5244936100097013,\n  'global_percentage_change': 0.5069367453780269,\n  'importance': 0.5636124084662236,\n  'metric': 'dietaryFats'}]"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "mims = anomalies[5][\"most_important_metrics\"]\n",
    "\n",
    "# We want the keys of each value to be the new keys, and the current keys to exist within the key of each new key, with the old values place there. Effectively we want to interchange the keys for the two levels.\n",
    "# dict((value, key) for value, key in zip(mims.values(), mims.keys()))\n",
    "\n",
    "top_level_keys = list(mims.values())[0].keys()\n",
    "inner_keys = list(mims.keys())\n",
    "\n",
    "output = {}\n",
    "\n",
    "# lowHeartRateEvents, weight, etc\n",
    "for top_level_key in list(mims.values())[0].keys():\n",
    "    output[top_level_key] = {}\n",
    "    # correlation, local_percentage_change, importance, etc\n",
    "    for inner_key in mims.keys():\n",
    "        output[top_level_key][inner_key] = mims[inner_key][top_level_key]\n",
    "\n",
    "label = 'metric'\n",
    "array = sorted(({**value, label:key } for key, value in zip(output.keys(), output.values())), key=lambda item: item[\"importance\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverts the inner and outer keys of a two-dimensional dictionary.\n",
    "def invert_dictionary(dictionary):\n",
    "    output = {}\n",
    "    for top_level_key in list(dictionary.values())[0].keys():\n",
    "        output[top_level_key] = {}\n",
    "        for inner_key in dictionary.keys():\n",
    "            output[top_level_key][inner_key] = dictionary[inner_key][top_level_key]\n",
    "    return output\n",
    "\n",
    "# Converts a dictionary into an array of values, sorted by a given key.\n",
    "def dict_to_array(dictionary, label, sorted_by_key):\n",
    "    return sorted(({**value, label:key } for key, value in zip(dictionary.keys(), dictionary.values())), key=lambda item: item[sorted_by_key], reverse=True)\n",
    "\n",
    "def fetch_most_important_metrics_related_to(all_data, subset_data, desired_metric, importance_threshold=0.4):\n",
    "\n",
    "    most_corr_metrics = fetch_most_correlated_to(subset_data, desired_metric)\n",
    "    global_percentage_change_for_subset = determine_global_percentage_change_for_subset(all_data, subset_data)\n",
    "    local_percentage_change_for_subset = determine_local_percentage_change_for_subset(subset_data)\n",
    "    joined_df = most_corr_metrics\n",
    "    joined_df[\"local_percentage_change\"] = local_percentage_change_for_subset\n",
    "    joined_df[\"global_percentage_change\"] = global_percentage_change_for_subset\n",
    "    joined_df.columns = [\"correlation\", \"local_percentage_change\", \"global_percentage_change\"]\n",
    "\n",
    "    # Drop the desired_metric column.\n",
    "    # joined_df = joined_df.drop(desired_metric, axis=0)\n",
    "\n",
    "    # Add an 'importance' column which weights each correlated metric by the percentage change which occured\n",
    "    # (both globally, and locally).\n",
    "    #\n",
    "    # We take into account the percentage change, as a correlation in and of itself isn't as useful out side\n",
    "    # of the context of a recent change. Indication of recent change would enable awareness and allow us to take action.\n",
    "    joined_df['importance'] = joined_df[\"correlation\"].abs() * joined_df[\"global_percentage_change\"].abs() * joined_df[\"local_percentage_change\"].abs()\n",
    "    joined_df['importance'] = joined_df['importance'] / joined_df['importance'].max()\n",
    "    sorted_df = joined_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "    # Separate the desired metric from the rest.\n",
    "    important_metrics, anomaly_metrics = sorted_df.drop(desired_metric, axis=0), sorted_df.loc[desired_metric]\n",
    "\n",
    "    return important_metrics, anomaly_metrics\n",
    "\n",
    "def fetch_anomalies_and_preceding_data(data, desired_metric=\"vitality\", preceding_num_days=7, std_deviations=1, importance_threshold=0.4):\n",
    "\n",
    "    if desired_metric is None:\n",
    "        desired_metric = \"vitality\"\n",
    "\n",
    "    if preceding_num_days is None:\n",
    "        preceding_num_days = 7\n",
    "\n",
    "    if std_deviations is None:\n",
    "        std_deviations = 1\n",
    "\n",
    "    if importance_threshold is None:\n",
    "        importance_threshold = 0.4\n",
    "\n",
    "    data = prepare_data(data)\n",
    "\n",
    "    anomalies = []\n",
    "\n",
    "    anomaly_idxs = detect_anomaly_indeces(data=data, desired_metric=desired_metric, std_deviations=std_deviations)\n",
    "\n",
    "    anomaly_values = [data[desired_metric].iloc[idx] for idx in anomaly_idxs]\n",
    "\n",
    "    for anomaly_index, anomaly_value in zip(anomaly_idxs, anomaly_values):\n",
    "\n",
    "        # The first element in the preceding data corresponds to the anomaly index. We add the 'startOfDate' as a top-level \n",
    "        # field to the returned dictionary.\n",
    "        preceding_data = get_data_preceding_anomaly(data, anomaly_index, preceding_num_days=preceding_num_days)\n",
    "\n",
    "        anomaly_row = preceding_data.iloc[0]\n",
    "        anomaly_start_of_date = anomaly_row[\"startOfDate\"]\n",
    "\n",
    "        # If the preceding data is empty for whatever reason, skip the current anomaly.\n",
    "        if preceding_data.empty:\n",
    "            log(f'Preceding data empty for anomaly index {anomaly_index} and value {anomaly_value}.')\n",
    "            continue\n",
    "            \n",
    "        # If we do not have enough preceding data to cover the preceding number of days, then we should \n",
    "        # skip this particular result.\n",
    "        if len(preceding_data) < preceding_num_days:\n",
    "            continue\n",
    "            \n",
    "        # Reverse and reindex the data.\n",
    "        preceding_data = preceding_data.reindex(index=preceding_data.index[::-1])\n",
    "        preceding_data_dict = pandas_to_array_dict(preceding_data)\n",
    "\n",
    "        important_metrics, anomaly_metrics = fetch_most_important_metrics_related_to(data, preceding_data, desired_metric)\n",
    "        most_important_metrics = important_metrics.loc[important_metrics[\"importance\"] >= 0.4]\n",
    "        most_important_metrics_dict = invert_dictionary(most_important_metrics.to_dict())\n",
    "        most_important_metrics_array = dict_to_array(dictionary=most_important_metrics_dict, label=\"metric\", sorted_by_key=\"importance\")\n",
    "\n",
    "        most_important_preceding_data = preceding_data[most_important_metrics.index.values]\n",
    "        most_important_preceding_data_dict = pandas_to_array_dict(most_important_preceding_data)\n",
    "\n",
    "        anomalies.append({\n",
    "            \"desired_metric\": desired_metric,\n",
    "            \"anomaly_index\": int(anomaly_index),\n",
    "            \"anomaly_value\": float(anomaly_value),\n",
    "            \"anomaly_metrics\": anomaly_metrics.to_dict(),\n",
    "            \"anomaly_start_of_date\": anomaly_start_of_date,\n",
    "            \"preceding_data\": preceding_data_dict,\n",
    "            \"most_important_metrics_dict\": most_important_metrics_dict,\n",
    "            \"most_important_metrics_array\": most_important_metrics_array,\n",
    "            \"most_important_preceding_data\": most_important_preceding_data_dict\n",
    "        })\n",
    "\n",
    "    # Sort the anomalies by date.\n",
    "    anomalies_sorted = sorted(anomalies, key=lambda anomaly: dateutil.parser.isoparse(anomaly[\"anomaly_start_of_date\"]), reverse=True)\n",
    "\n",
    "    return anomalies_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomalies = fetch_anomalies_and_preceding_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'2020-06-25T00:00:00.000Z'"
     },
     "metadata": {},
     "execution_count": 177
    }
   ],
   "source": [
    "anomalies[5][\"anomaly_start_of_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'correlation': 1.0,\n 'local_percentage_change': 1.0211046250561298,\n 'global_percentage_change': 0.04119081410933867,\n 'importance': 0.12895636978312774}"
     },
     "metadata": {},
     "execution_count": 167
    }
   ],
   "source": [
    "anomalies[0][\"anomaly_metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "datetime.datetime(2020, 7, 4, 0, 0, tzinfo=tzutc())"
     },
     "metadata": {},
     "execution_count": 162
    }
   ],
   "source": [
    "import dateutil\n",
    "start_of_date = anomalies[0][\"anomaly_start_of_date\"]\n",
    "parsed = dateutil.parser.isoparse(start_of_date)\n",
    "parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting anomaly indeces by a given date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def detect_anomaly_indeces(data, desired_metric, std_deviations=1, date=None):\n",
    "\n",
    "    sigma = data[desired_metric].std()\n",
    "    mu = data[desired_metric].mean()\n",
    "\n",
    "    upper_bound = mu + sigma*std_deviations\n",
    "    lower_bound = mu - sigma*std_deviations\n",
    "\n",
    "    if date is None:\n",
    "        anomaly_indeces = np.where((data[desired_metric] != 0) & (data[desired_metric] < lower_bound) | (data[desired_metric] > upper_bound))\n",
    "    else: \n",
    "        anomaly_indeces = np.where(\n",
    "            (data[desired_metric] != 0) & \n",
    "            ((data[desired_metric] < lower_bound) | (data[desired_metric] > upper_bound)) &\n",
    "            (data[\"startOfDate\"] == date)\n",
    "        )\n",
    "\n",
    "    return anomaly_indeces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([5])"
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "detect_anomaly_indeces(data, \"mood\", date=\"2020-07-01T00:00:00.000Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "startOfDate                                      2020-07-01T00:00:00.000Z\ngeneralFeeling                                                          2\nmood                                                                    2\nenergy                                                                  1\nfocus                                                                   1\nattributesAndCounts     {'generalFeeling': 1, 'mood': 1, 'energy': 1, ...\nactiveEnergyBurned                                                485.684\nbasalEnergyBurned                                                 1576.85\ncaloricIntake                                                     1413.21\ndietaryCarbohydrates                                              108.996\ndietaryFats                                                       54.6279\ndietaryProtein                                                    121.065\nhrv                                                               76.0992\nlowHeartRateEvents                                                     10\nrestingHeartRate                                                       36\nsleepHours                                                        6.36931\nweight                                                                  0\nName: 5, dtype: object"
     },
     "metadata": {},
     "execution_count": 152
    }
   ],
   "source": [
    "data.iloc[5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}